# Big-Data-Spark-Project-1

# Wikipedia Big Data Analysis

This analysis consists of using big data tools to answer questions about datasets from Wikipedia. There are a series of analysis questions, answered using Hive and MapReduce. The tools used are determined based on the context for each question.

# Technologies Used

1.  Hadoop
2.  HDFS
3.  Python
4.  Hive
5.  Sqoop
6.  MapReduce
7.  Yarn
8.  Git + Github

# Features

1.  Find, organize, and format pageviews on any given day.
2.  Follow clickstreams to find relative frequencies of different pages.
3.  Determine relative popularity of page access methods.
4.  Compare yearly popularity of pages.

