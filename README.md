# Big-Data-Spark-Project-1

# Wikipedia Big Data Analysis

This analysis consists of using big data tools to answer questions about datasets from Wikipedia. There are a series of analysis questions, answered using Hive and MapReduce. The tools used are determined based on the context for each question.

# Technologies Used

1.  Hadoop
2.  HDFS
3.  Python
4.  Hive
5.  MapReduce
6.  Yarn
7.  Git + Github

# Features

1.  Find, organize, and format pageviews on any given day.
2.  Determine relative popularity of page access methods.
3.  Compare yearly popularity of pages.
4.  Find the different way to analyze the most internal search link fraction of hotel california.

# Problem Statement

1. Which English wikipedia article got the most traffic on January 20, 2021? 
2. What English wikipedia article has the largest fraction of its readers follow an internal link to another wikipedia article?
3. What series of wikipedia articles, starting with Hotel California, keeps the largest fraction of its readers clicking on internal links.
4. Find an example of an English wikipedia article that is relatively more popular in the Americas than elsewhere.There is no location data associated with the wikipedia pageviews    data, but there are timestamps. You'll need to make some assumptions about internet usage over the hours of the day.
5. Difference between total views of 'en' and 'en.m' pages.

# Data Set used

Pageviews Filtered to Human Traffic
    https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Traffic/Pageviews
Monthly Clickstream
    https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream
